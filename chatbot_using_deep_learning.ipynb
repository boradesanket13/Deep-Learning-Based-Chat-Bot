{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Deep Learning Project Based Learning Group 7 (Roll no 9-12)**\n",
        "\n",
        "**Title**: Develop a Chatbot using Deep Learning Algorithms\n"
      ],
      "metadata": {
        "id": "4OnwlyjtvDvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Installation"
      ],
      "metadata": {
        "id": "ZrAoDP1zuzLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qiOb6jFY2m9C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6d591b3f-b0fb-41b3-d814-81e6e2294bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#Installing the libraries:\n",
        "!pip install numpy tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Preparing the Training Data"
      ],
      "metadata": {
        "id": "0tTUST2gwaL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SpMaUMol3GGl"
      },
      "outputs": [],
      "source": [
        "# Training data (user messages)\n",
        "train_data = [\n",
        "    \"Hello\",\n",
        "    \"How are you?\",\n",
        "    \"Good morning\",\n",
        "    \"Good evening\",\n",
        "    \"Nice to meet you\",\n",
        "    \"What's up?\",\n",
        "    \"How's your day going?\",\n",
        "    \"Greetings!\",\n",
        "    \"Good afternoon\",\n",
        "    \"How can I assist you?\",\n",
        "    \"Pleasure to see you\",\n",
        "    \"Is there anything I can help with?\",\n",
        "    \"What are you up to?\",\n",
        "    \"How's the weather today?\",\n",
        "    \"Long time no see!\",\n",
        "    \"Hey there!\",\n",
        "    \"What have you been doing?\",\n",
        "    \"How was your weekend?\",\n",
        "    \"Tell me about yourself.\",\n",
        "    \"What's your favorite hobby?\",\n",
        "    \"Do you like music?\",\n",
        "    \"Any plans for today?\",\n",
        "    \"Can you recommend a good book?\",\n",
        "    \"What's the meaning of life?\",\n",
        "    \"How do you define happiness?\",\n",
        "    \"Are you a robot?\",\n",
        "    \"Where are you from?\",\n",
        "    \"What languages do you speak?\",\n",
        "    \"What's the capital of France?\",\n",
        "]\n",
        "\n",
        "# Output data (bot responses)\n",
        "train_labels = [\n",
        "    \"Hi\",\n",
        "    \"I'm fine, how about you?\",\n",
        "    \"Good morning to you\",\n",
        "    \"Good evening, how can I help you?\",\n",
        "    \"Nice to meet you too\",\n",
        "    \"Not much, just hanging out\",\n",
        "    \"It's going well, thank you\",\n",
        "    \"Hello!\",\n",
        "    \"Good afternoon to you too\",\n",
        "    \"I'm here to assist you\",\n",
        "    \"Likewise!\",\n",
        "    \"Yes, I have a question\",\n",
        "    \"Just chilling, thanks for asking!\",\n",
        "    \"It's sunny today, perfect weather!\",\n",
        "    \"Indeed, it's been a while!\",\n",
        "    \"Hey! How can I assist you today?\",\n",
        "    \"Just working on improving myself!\",\n",
        "    \"It was relaxing, thanks for asking!\",\n",
        "    \"Sure, I'm a chatbot designed to assist you!\",\n",
        "    \"I enjoy chatting with users like you!\",\n",
        "    \"Yes, I'm programmed to appreciate music!\",\n",
        "    \"Nothing specific, just here to help you out!\",\n",
        "    \"Certainly! How about 'The Great Gatsby'?\",\n",
        "    \"The meaning of life is subjective, what's your take?\",\n",
        "    \"Happiness is feeling fulfilled and content.\",\n",
        "    \"I'm an AI-powered chatbot!\",\n",
        "    \"I exist in the digital realm!\",\n",
        "    \"I speak the language of ones and zeros!\",\n",
        "    \"The capital of France is Paris!\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "  \n",
        "1.   Label Encoding\n",
        "2.   Tokenisation\n",
        "3.   Padding"
      ],
      "metadata": {
        "id": "_qf9M9ebxLOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qOqUcMHh3MDf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Label Encoding\n",
        "# 1.1 The LabelEncoder is fitted on the train_labels to learn the unique labels and assign them numerical values.\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 1.2 The transform method is then used to encode the labels.\n",
        "encoded_labels = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "# 2. Tokenisation\n",
        "# 2.1 The Tokenizer is fitted on the train_data to learn the unique words and assign them integer values.\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "# 2.2 The texts_to_sequences method is used to convert the text data into sequences of integers based on the learned mapping.\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
        "\n",
        "# 3. Padding\n",
        "# 3.1 pad_sequences method is used to ensure all sequences have the same length by padding or truncating them.\n",
        "train_sequences = keras.preprocessing.sequence.pad_sequences(train_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 3: Building and Training the Chatbot Model\n",
        "\n",
        "**Layers:**\n",
        "\n",
        "**1.   Embedding Layer**: Maps the input sequence of integers to a dense vector representation.\n",
        "\n",
        "**2.   Flaten Layer**: Convert the multi-dimensional output of the Embedding layer into a one-dimensional vector\n",
        "\n",
        "**3.   Dense layer with ReLU activation**:  Introduces non-linearity to the model, allowing it to learn complex patterns in the data.\n",
        "\n",
        "**4.   Output Layer with Softmax Activation Function**: Produces probability scores for each class label\n",
        "\n",
        "\n",
        "**Compile the model using**\n",
        "1. Adam optimizer\n",
        "2. Sparse categorical crossentropy loss function\n",
        "\n",
        "**Training model using fit method with following parameters**\n",
        "1. input sequences\n",
        "2. encoded labels\n",
        "3. epochs (number of times the model will iterate over the entire training dataset)"
      ],
      "metadata": {
        "id": "Vb9hT1qZ0evV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YfCSOkfz3ONH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "be763245-f96b-411a-8334-3e5a3e25fa63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.3738 - accuracy: 0.0345\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.3493 - accuracy: 0.1379\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3262 - accuracy: 0.3103\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3038 - accuracy: 0.3448\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.2819 - accuracy: 0.3448\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.2595 - accuracy: 0.4483\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.2364 - accuracy: 0.5172\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.2122 - accuracy: 0.5517\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.1870 - accuracy: 0.6207\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1609 - accuracy: 0.7241\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.1338 - accuracy: 0.7586\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.1061 - accuracy: 0.7586\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0772 - accuracy: 0.7586\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.0468 - accuracy: 0.7586\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.0151 - accuracy: 0.7586\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9822 - accuracy: 0.7931\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9479 - accuracy: 0.7931\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.9121 - accuracy: 0.7931\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8749 - accuracy: 0.7931\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.8363 - accuracy: 0.7931\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7962 - accuracy: 0.7931\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7544 - accuracy: 0.7931\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7110 - accuracy: 0.8276\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6660 - accuracy: 0.8621\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6192 - accuracy: 0.8621\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5707 - accuracy: 0.8621\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.5206 - accuracy: 0.9310\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4690 - accuracy: 0.9310\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4157 - accuracy: 0.9310\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3606 - accuracy: 0.9655\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3039 - accuracy: 0.9655\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2455 - accuracy: 0.9655\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1857 - accuracy: 0.9655\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1245 - accuracy: 0.9655\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0619 - accuracy: 0.9655\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9981 - accuracy: 0.9655\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9332 - accuracy: 0.9655\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.8671 - accuracy: 0.9655\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8001 - accuracy: 0.9655\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7324 - accuracy: 0.9655\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6640 - accuracy: 0.9655\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5951 - accuracy: 0.9655\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5259 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4565 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.3872 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3182 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2495 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1820 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1155 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0500 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f8ac9947a90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# We will use an Embedding layer to handle the text data.\n",
        "# The Embedding layer maps the input sequence of integers to a dense vector representation.\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# The Embedding layer takes two arguments:\n",
        "# 1. Vocabulary size given by (len(tokenizer.word_index) + 1), which is determined by the total number of unique words in our training data\n",
        "# 2. Embedding dimension (100 in this case), which specifies the size of the dense vector representation for each word.\n",
        "model.add(keras.layers.Embedding(len(tokenizer.word_index) + 1, 100, input_length=train_sequences.shape[1]))\n",
        "\n",
        "# The Flatten layer is added to convert the multi-dimensional output of the Embedding layer into a one-dimensional vector.\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# The dense layer with ReLU activation introduces non-linearity to the model, allowing it to learn complex patterns in the data.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Output Layer with Softmax Activation Function: Produces probability scores for each class label\n",
        "model.add(keras.layers.Dense(len(train_labels), activation='softmax'))\n",
        "\n",
        "# Compile the model using the Adam optimizer and the sparse categorical crossentropy loss function, which is suitable for multi-class classification problems.\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training model using fit method with parameters (input sequences, encoded labels, epochs)\n",
        "model.fit(train_sequences, encoded_labels, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generating Responses"
      ],
      "metadata": {
        "id": "KWSdKFwX7KE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yZlKl6_P3jOM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# takes input \"text\"\n",
        "def generate_response(text):\n",
        "\n",
        "    # tokenizes the input\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # pads the sequence\n",
        "    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=train_sequences.shape[1])\n",
        "\n",
        "    # makes predictions using the trained model\n",
        "    prediction = model.predict(sequence)\n",
        "\n",
        "    # predicted label is then converted back into its original text form using the inverse_transform method of the LabelEncoder\n",
        "    predicted_label = np.argmax(prediction)\n",
        "    response = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bniNQ7E3mOC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "065bbb74-ea5f-45cf-b2d6-da71478b5959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a message: Hello\n",
            "1/1 [==============================] - 0s 233ms/step\n",
            "ChatBot:  Hi\n",
            "Enter a message: Tell me about yourself\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "ChatBot:  Sure, I'm a chatbot designed to assist you!\n",
            "Enter a message: What are you up to?\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "ChatBot:  Just chilling, thanks for asking!\n",
            "Enter a message: Do you like music?\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "ChatBot:  Yes, I'm programmed to appreciate music!\n",
            "Enter a message: What's the capital of France?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "ChatBot:  The capital of France is Paris!\n",
            "Enter a message: What languages do you speak?\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "ChatBot:  I speak the language of ones and zeros!\n",
            "Enter a message: Can you recommend a good book?\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "ChatBot:  Certainly! How about 'The Great Gatsby'?\n",
            "Enter a message: weather today?\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "ChatBot:  Hi\n"
          ]
        }
      ],
      "source": [
        "# continuously prompts the user for input and generates responses using the generate_response function\n",
        "while True:\n",
        "    user_input = input(\"Enter a message: \")\n",
        "    response = generate_response(user_input)\n",
        "    print(\"ChatBot: \", response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtO_byk43m4J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}